resources:
  jobs:
    demo_workflow:
      name: demo_workflow_${bundle.target}
      
      tasks:
        - task_key: task_1_data_preparation
          notebook_task:
            notebook_path: ../src/task_1_data_preparation.py
            source: WORKSPACE
          job_cluster_key: shared_cluster
        
        - task_key: task_2_data_analysis
          depends_on:
            - task_key: task_1_data_preparation
          notebook_task:
            notebook_path: ../src/task_2_data_analysis.py
            source: WORKSPACE
          job_cluster_key: shared_cluster
      
      # Define shared classic compute cluster for all tasks
      job_clusters:
        - job_cluster_key: shared_cluster
          new_cluster:
            spark_version: "13.3.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            num_workers: 1
            spark_conf:
              "spark.databricks.delta.preview.enabled": "true"
            custom_tags:
              ResourceClass: "Classic"
      
      # Schedule to run daily at 9 AM UTC
      schedule:
        quartz_cron_expression: "0 0 9 * * ?"
        timezone_id: "UTC"
        pause_status: "PAUSED"
      
      # Email notifications
      email_notifications:
        on_failure:
          - kunal.marwah@databricks.com
      
      # Maximum concurrent runs
      max_concurrent_runs: 1
      
      # Timeout in seconds (15 minutes for cluster startup + execution)
      timeout_seconds: 900
      
      tags:
        environment: "${bundle.target}"
        project: "dab_demo"

